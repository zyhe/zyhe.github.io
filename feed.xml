<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zyhe.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://zyhe.github.io/" rel="alternate" type="text/html" /><updated>2025-02-02T20:52:02+00:00</updated><id>https://zyhe.github.io/feed.xml</id><title type="html">Zhiyu He</title><subtitle>Homepage of Zhiyu He</subtitle><author><name>Zhiyu He</name></author><entry><title type="html">西游之悟：灵山只在汝心头</title><link href="https://zyhe.github.io/2025/02/02/journey-to-the-west.html" rel="alternate" type="text/html" title="西游之悟：灵山只在汝心头" /><published>2025-02-02T20:00:00+00:00</published><updated>2025-02-02T20:00:00+00:00</updated><id>https://zyhe.github.io/2025/02/02/journey-to-the-west</id><content type="html" xml:base="https://zyhe.github.io/2025/02/02/journey-to-the-west.html"><![CDATA[<h1 id="西游之悟灵山只在汝心头">西游之悟：灵山只在汝心头</h1>

<p>何志宇 2025年2月2日</p>

<p>《西游记》以唐僧师徒西往灵山求取真经为主线，融奇绝瑰丽的想象和幽微细致的洞察于一体，书写了一部异彩纷呈的史诗。传世名著既拥有丝丝入扣的故事情节，又有对人性和社会的细腻反映，更因其深邃内涵而兼具多种角度的解读。</p>

<p>通过西天取经这一主线，作者流畅地串起了唐僧师徒所经历的多元化的冒险经历。这些跌宕起伏的情节也成为了后世众多流行文化作品的重要基础，如周星驰的《大话西游》系列，以及《黑神话：悟空》游戏。天庭也有层次化的权力结构，其中的各路神仙既为取经团队提供护佑，也直接或间接地制造了各类困难；呈多元化构成的取经团队里也有分歧、合作、牺牲和互利。古往今来，人们从道佛、讽喻、玩世、劝学和心学等诸多视角来剖析西游宇宙，阐发其深邃意蕴。</p>

<p>我想以第八十五回中悟空的“佛在灵山莫远求，灵山只在汝心头”一语为统摄，就修炼成长的角度来分析西游记提供的智慧。无论是个体还是群体，都会在一段或长或短的阶段里，发愿做一番可大可小的事业。为实现目标，需要发挥各方面的潜能，逢山开路，遇水搭桥，必要时寻求内外部帮助，从而克服横亘在前行路上的重重难关，最终才能功成圆满，见性明心。但这所有的一切，都依赖于坚定、强大、平和而沉稳的内心。</p>

<div style="text-align: center; margin-top: 20px; margin-bottom: 30px;">
  <img src="/img/posts/2025-2-2-Journey-to-the-west/hills.avif" alt="目标" style="width: 60%; display: block; margin: 0 auto;" />
</div>

<!-- 名著往往触及了最本质的方面，提供了最深刻的描绘。同时，由于他们触及了重要的母题，因此具有多种解读角度。例如，天庭是对一般化的权力结构的抽象；各路妖魔鬼怪中，也有亲疏之分，他们的结局也各不相同。我想就取经本身，探讨修炼的意义。
西游记可被视为个人/团队修行或长之书。初为发展做一番前无古人（后亦难有来者）的大事业，继而为寻找帮手、亦可视为锻炼各方面的能力，随后便是逢山开路，遇水搭桥，克服一个又一个的困难。最后才是功成圆满，见性明心。 -->

<h2 id="独自修行不可替代的成长之路">独自修行：不可替代的成长之路</h2>

<p>唐僧的各位弟子都是“谪仙”，能够腾云驾雾，各有神通。唐僧本人虽贵为如来之徒金蝉子转生，无奈因肉体凡胎而力犹未逮。既然悟空往返灵山只是弹指一瞬，为何不由他携唐僧直至佛祖座前求取真经呢？</p>

<p>作者在第二十二回借八戒之问和悟空之答做了巧妙的回应，既让整个取经故事变得自洽，也阐发了坚持修炼的意义。八戒问道：“哥啊，既是这般容易，你把师父背著，只消点点头，躬躬腰，跳过去罢了，何必苦苦的与这怪厮战？”悟空作答：</p>

<!-- 在取经团队中，唐僧的三个弟子都能腾云驾雾，大有神通。那为什么不由悟空直接携唐僧从东土大唐到佛祖座前取得真经呢？八戒也有这般疑问，他说：“哥啊，既是这般容易，你把师父背著，只消点点头，躬躬腰，跳过去罢了，何必苦苦的与这怪厮战？”悟空所做的回答，既让整个取经故事变得自洽，也点出了修炼的意义。他说： -->

<blockquote>
  <p>“但只是师父要穷历异邦，不能够超脱苦海，所以寸步难行也。我和你只做得个拥护，保得他身在命在，替不得这些苦恼，也取不得经来；就是有能先去见了佛，那佛也不肯把经善与你我。正叫做‘若将容易得，便作等闲看’。”</p>
</blockquote>

<p>原来，漫漫取经之路是唐僧师徒为获真经而必经的修炼，既无从逃避，亦无捷径可循。唐僧固然可借助弟子的神通来省却无限烦恼，径往西天。然而，若缺失了这一路上必要的磨练，便会与真经缘悭一面，无法真正开悟。</p>

<p>有一种诱惑是“如果”：如果能借助别人的力量来直接做成，那能有多好！然而，这种唾手可得的收获会随时间推移而被我们快速忽视。此外，我们不总有倚仗他人的机会，也没有人能代为克服属于我们自己的各项挑战。反之，若能经过奋斗而实现足够有挑战性的目标，我们才会从中获得极大的慰藉。</p>

<div style="text-align: center; margin-top: 20px; margin-bottom: 30px;">
  <img src="/img/posts/2025-2-2-Journey-to-the-west/buddha.avif" alt="如来" style="width: 60%; display: block; margin: 0 auto;" />
</div>

<!-- 人们常说“目标不能轻易实现”。一方面，足够有挑战性的目标才能激励我们为之奋斗；另一方面，我们也会忽视。 -->

<!-- 每个人都要独自面对生活中的各项挑战，别人替代不得。 -->

<h2 id="信念的力量挺过至暗时刻">信念的力量：挺过至暗时刻</h2>

<p>修炼不仅仅是独自面对挑战，坚定的信念同样至关重要，也是激励我们持续前行的关键。不然，各类困难会令人未战先怯，以至不能充分发挥潜能，甚至半途而废。</p>

<!-- 这也是为什么毛泽东能在抗日战争处于初期的不利阶段摒弃速胜论、速败论等极端论调、而得出清醒的论持久战的原因。 -->

<p>值得引以为戒的是，即便强如悟空，也会有信念动摇的至暗时刻。悟空降妖除怪，从未有丝毫畏惧或退却的念头，直到他在狮驼岭中三魔大鹏的诛心之计时。</p>

<p>当狮驼岭众魔擒获唐僧师徒而走脱悟空后，心狠手辣的大鹏提出先将唐僧藏匿，再放出唐僧已被吞食的谣言在狮驼城中传播，从而让悟空死心塌地放弃营救唐僧。果不其然，悟空在误以为唐僧已死后心如刀绞，万念俱灰。因为误以为取经之梦将永远幻灭，他叹到“西方胜境无缘到，气散魂消怎奈何？”，便至如来前哭诉，甚至打算重回水帘洞。好在经如来点醒和施展法力，唐僧一众终于获救。悟空等也重新找回了失去的信念，继续踏上了前往灵山的路途。</p>

<!-- 在后半部中，作者用四回详细叙述了唐僧师徒挺过狮驼岭重重磨难的遭遇。这里有一位强劲的敌手：从辈分上说要数如来之舅的大鹏。在利用计谋擒得除悟空外的其它师徒后，为应对悟空威胁，他使出了一条诛心之计： 
“我这皇宫里面有一座锦香亭子，亭子内有一个铁柜。依著我，把唐僧藏在柜里，关了亭子；却传出谣言，说唐僧已被我们夹生吃了，令小妖满城讲说。那行者必然来探听消息，若听见这话，他必死心塌地而去。待三五日不来搅扰，却拿出来，慢慢受用。如何？” -->
<!-- 果然，悟空心如刀绞，万念俱灰，他想到“西方胜境无缘到，气散魂消怎奈何？”，最终打定主意要见一见如来，如若不能取得真经，便重回水帘洞。 -->
<!-- 当狮驼岭师徒四人均落难时，大鹏的毒计使悟空误以为唐僧已经遇害，他心神恍惚，极度不平，总算在如来的指点下才看清真相，找回了失去的信念。 -->

<p>假如我们有外部的肯定和支持，那当然最好。但在更多时候，要用坚定的信念来提供持续的动力，在任何时候都不要放弃希望。当然，这种信念不应是盲目的，而是在经审时度势、缜密分析和去芜存菁后所做出的准确判断。
<!-- 智若能在审时度势后提取关键事信息，做出准确判断。 --></p>

<div style="text-align: center; margin-top: 20px; margin-bottom: 30px;">
  <img src="/img/posts/2025-2-2-Journey-to-the-west/monster.webp" alt="狮驼岭" style="width: 45%; display: block; margin: 0 auto;" />
</div>

<h2 id="戒骄戒躁心如止水">戒骄戒躁，心如止水</h2>

<p>纵使笃定如唐僧，也难免会在山高路远时为目标难及而骄躁。当望见崇山峻岭时，他会道“悟空，你说得几时方可到？”，也会说：“休言无事。我看那山峰挺立，远远的有些凶气，暴云飞出，渐觉惊惶，满身麻木，神思不安。” 悟空所答颇有禅机，他说：</p>

<blockquote>
  <p>“你自小时走到老，老了再小，老小千番也还难；只要你见性志诚，念念回首处，即是灵山。”</p>

  <p>“心净孤明独照，心存万境皆清。差错些儿成惰懈，千年万载不成功。但要一片志诚，雷音只在眼下。似你这般恐惧惊惶，神思不安，大道远矣，雷音亦远矣。且莫胡疑，随我去。”</p>
</blockquote>

<p>须知，浮躁时产生的想法和做出的决定往往是不可靠的。要在大多数时候争取心态平和，也定要能戒骄戒躁，脚踏实地。
<!-- 当然，在冲刺等重要时刻，需调动精神，全力以赴。   --></p>

<div style="text-align: center; margin-top: 20px; margin-bottom: 30px;">
  <img src="/img/posts/2025-2-2-Journey-to-the-west/mountain.avif" alt="山峰" style="width: 45%; display: block; margin: 0 auto;" />
</div>

<!-- 当然，就故事情节的连贯性和多样性而言，西游记并不如水浒传那般精彩。在西游记中，各回得故事相对独立，且形式单一，一般是唐僧（或与八戒及沙说）一起蒙难，仰仗悟空解救，而悟空常力犹未逮，便需天庭的其它神仙一并助力。 -->

<h2 id="结语灵山只在汝心头">结语：灵山只在汝心头</h2>

<p>西游记不仅仅是一部情节跌宕的文学巨著，也因其探讨的修炼成长的主题而引人深思。梦在前方，路在脚下，就让坚定而沉稳的内心引领我们到达灵山胜境。</p>]]></content><author><name>Zhiyu He</name></author><summary type="html"><![CDATA[西游之悟：灵山只在汝心头]]></summary></entry><entry><title type="html">Gittins Index Theorem and Multi-Armed Bandits</title><link href="https://zyhe.github.io/2024/12/05/Gittins-index.html" rel="alternate" type="text/html" title="Gittins Index Theorem and Multi-Armed Bandits" /><published>2024-12-05T11:00:00+00:00</published><updated>2024-12-05T11:00:00+00:00</updated><id>https://zyhe.github.io/2024/12/05/Gittins%20index</id><content type="html" xml:base="https://zyhe.github.io/2024/12/05/Gittins-index.html"><![CDATA[<h1 id="gittins-index-theorem-and-multi-armed-bandits">Gittins Index Theorem and Multi-Armed Bandits</h1>

<p>Zhiyu He, December 5, 2024<br />
Adapted from</p>

<ul>
  <li>Gittins, John, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John Wiley &amp; Sons, 2011.</li>
  <li>Sébastie Bubeck and Nicolò Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning, 2012.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Goal: solve sequential decision-making problems featuring exploration-exploitation trade-offs</li>
  <li>Salient feature: to find an optimal policy, it suffices to focus on an index corresponding to each choice and encompassing immediate rewards and future benefits</li>
</ul>

<h3 id="motivating-example">Motivating Example</h3>

<ul>
  <li>Two-armed bandit<br />
Consider two slot machines that give the following reward sequences
    <ul>
      <li>Slot machine 1: $4, 5, 2, 6, 10, 1, \ldots$</li>
      <li>Slot machine 2: $8, 3, 10, 12, 1, 2, \ldots$</li>
    </ul>

    <p>If not pulled, then the reward to be given is fixed.<br />
How to sequentially select a machine (i.e., an arm) to maximize the discounted cumulative reward?<br />
A myopic solution: $8 + 4a + 5a^2 + 3a^3 + 10a^4 + \ldots$<br />
An arguably better solution $8 + 3a + 10a^2 + 12a^3 + 4a^4 + \ldots$<br />
(For instance, if the discount factor is $a=0.9$, then the latter discounted sum is larger than the former)</p>
  </li>
</ul>

<h3 id="three-formulations-of-the-bandit-problem">Three formulations of the bandit problem</h3>

<ul>
  <li>Criterion: the nature of the reward sequence
    <ul>
      <li>stochastic (drawn from some unknown distribution)
-&gt; upper confidence bound strategies<br />
idea: construct upper confidence bound estimates on the means of arms and choose the arm that looks best under these estimates<br />
principle: optimism in face of uncertainty</li>
      <li>adversarial (arbitrarily set after we select the arm)
-&gt; hedge algorithm<br />
idea: maintain and iteratively update a probability distribution of selecting different arms</li>
      <li>Markovian (features known state transition)<br />
-&gt; Gittins index</li>
    </ul>
  </li>
</ul>

<h2 id="problem-formulation">Problem Formulation</h2>

<ul>
  <li>
    <p>Simple family of alternative bandit processes<br />
$n$ independent bandit processes $B_i, i=1,\ldots,N$</p>
  </li>
  <li>two possible actions
    <ul>
      <li>continue: produces reward and causes state transition</li>
      <li>freeze: no reward and the state is fixed</li>
    </ul>
  </li>
  <li>
    <p>state $\xi_i$ lies in a countable state space $E_i$</p>
  </li>
  <li>
    <p>At decision time $t$: apply control to a specific $B_i$</p>
  </li>
  <li>Results
    <ul>
      <li>obtain a reward $r_i(\xi_i)$ from the bandit process $B_i$</li>
      <li>the state of $B_i$ transits from $\xi_i$ to $y$ with probability $P_i(y|\xi_i)$</li>
      <li>other bandits are frozen: their states do not change, and no rewards are obtained</li>
    </ul>
  </li>
  <li>
    <p>Goal: maximize the infinite-horizon expected discounted sum of rewards (i.e., the payoff)</p>

\[\sup_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} a^t r_{i_t} (\xi_{i_t} (t))\right]\]

    <p>$a \in (0,1)$: discount factor   $i_t$: the index of the bandit process chosen at time $t$</p>
  </li>
</ul>

<h3 id="why-not-alternative-methods">Why not alternative methods?</h3>

<ul>
  <li>Dynamic programming: the number of stationary policies (or the size of the state space) grows exponentially with the number of bandits
    <ul>
      <li>
        <p>Value function</p>

\[\quad V(\xi) = \sup_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} a^t r_{i_t} (\xi_{i_t} (t)) \mathrel{\Big|} \xi(0) = \xi \right]\]
      </li>
      <li>
        <p>Bellman’s equation</p>

\[V(\xi) = \max_{i \in \{1, \ldots, n\}} \left\{ r_i(\xi_i) + a \sum_{y \in E_i} P_i(y|\xi_i)V(\xi_1, \ldots, \xi_{i-1}, y, \xi_{i+1}, \ldots, \xi_n) \right\}\]
      </li>
    </ul>

    <p>number of states $\prod_i k_i \triangleq k$   for each state, we need to choose from $n$ actions</p>

    <p>number of stationary policies: $k^n$</p>
  </li>
</ul>

<h2 id="gittins-index-theorem">Gittins Index Theorem</h2>
<!-- ### Intuitions from single machine scheduling -->

<ul>
  <li>
    <p>Key idea: we aim to find an index indicating the behavior of a bandit process. This behavior blends immediate rewards and future benefits.</p>
  </li>
  <li>
    <p>Point of attack: construct a simple reference bandit process</p>
  </li>
</ul>

<h3 id="calibrating-bandit-processes">Calibrating bandit processes</h3>

<ul>
  <li>
    <p>Consider a discrete-time standard bandit process $\Lambda$, which gives a known reward $\lambda$ each time it is continued</p>

    <p>If it is selected at every time, then the cumulative discounted reward is</p>

\[\lambda(1+a+a^2+\ldots) = \frac{\lambda}{1-a}\]

    <p>We want to compare a bandit process $B_i$ and $\Lambda$. Consider the following setting: we continuously apply control to $B_i$ and then follow an optimal policy.</p>

    <p>This policy can switch to $\Lambda$ at some future time $\tau(\tau&gt;0)$</p>

    <ul>
      <li>
        <p>Observation: if an optimal policy causes us to switch at time $\tau$, we will never switch back</p>
      </li>
      <li>
        <p>Reason: the information on $B_i$ at time $\tau + 1$ is the same as at time $\tau$. If applying control to $\Lambda$ is optimal at time $\tau$, then it is also optimal at time $\tau + 1$ and afterwards.</p>
      </li>
    </ul>

    <p>The maximal payoff is</p>

\[\sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) + a^{\tau} \frac{\lambda}{1-a} \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>We want to find $\lambda$ such that it is equally optimal to apply control to either of the two bandit processes initially, i.e.,</p>

\[0 = \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) - (1 - a^{\tau}) \frac{\lambda}{1-a} \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>We note that $\sum_{t=0}^{\tau-1} a^\tau = \frac{1-a^\tau}{1-a}$. Therefore, $\lambda$ satisfies the following equation</p>

\[0 = \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} (a^t r_i(x_i(t)) - a^t \lambda) \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>The right-hand side is the supremum of decreasing linear functions of $\lambda$. Hence, it is convex and decreasing in $\lambda$. The root exists and is unique. We obtain the following expression of $\lambda$</p>

\[\lambda = \sup_{\tau &gt; 0} \frac{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) \right | x_i(0)=x_i ]}{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t | x_i(0)=x_i \right] }\]

    <p>Another equivalent formulation</p>

\[\sup\left\{\lambda : \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t [r_i(x_i(t)) - \lambda] \mid x_i(0) = x_i \right] \geq 0\right\}\]

    <p>Reason: the supremum $\lambda$ ensures that the expectation equals zero.</p>

    <p>Interpretation of $\lambda$</p>
    <ul>
      <li>fixed reward that makes us indifferent to which of $B_i$ and $\Lambda$ to continue initially</li>
      <li>fair charge (highest rent) that we are willing to pay to continue $B_i$ for at least one round and then optimally switch</li>
    </ul>

    <p>Observation</p>
    <ul>
      <li>with the supreme $\lambda$ and the optimal stopping time $\tau$, the cumulative discounted rewards collected from these bandit processes are the same</li>
    </ul>

    <p>This $\lambda$ is the so-called Gittins index</p>
  </li>
</ul>

<h3 id="key-statement">Key statement</h3>

<p>We maximize the expected discounted reward obtained from a simple family of alternative bandit processes by always continuing the bandit having greatest Gittins index. Namely, for</p>

\[v(B_i, x_i) = \sup_{\tau &gt; 0} \frac{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) \right | x_i(0)=x_i ]}{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t | x_i(0)=x_i \right] }\]

<p>we always select the bandit process $i = \operatorname{argmax}_i v(B_i,x_i)$.</p>

<ul>
  <li>Numerator: discounted reward up to $\tau$ (stopping time)</li>
  <li>Denominator: discounted time up to $\tau$</li>
</ul>

<p>Note that the Gittins index only depends on the corresponding bandit process (or more specifically, $a, r_i, p_i$) but not on other processes.</p>

<h3 id="proof-via-prevailing-charges-arguments">Proof via prevailing charges arguments</h3>

<ul>
  <li>
    <p>Prevailing charge $g_i(x_i)$: fixed charge that we pay to continue playing the bandit $i$ at state $x_i$<br />
If $g_i(x_i)$ is too large, then we will quit playing this bandit</p>
  </li>
  <li>
    <p>Fair charge $\lambda_i(x_i)$: level of prevailing charge when we are indifferent at state $x_i$ between stopping and continuing for at least one more step and then optimally stopping<br />
We give $\lambda_i$ in the previous ratio</p>
  </li>
</ul>

<p>Processes</p>

<ul>
  <li>Initially, $g_{i,0} = \lambda_i(x_i(0))$</li>
  <li>A point will be reached when it would be optimal to stop continuing the bandit $i$. This point corresponds to the stopping time $\tau$.
At this time, $\lambda_i(x_i(\tau)) &lt; g_{i,0} = \lambda_i(x_i(0))$
This is also the first time that the prevailing charge is higher than the fair charge</li>
  <li>We reduce the prevailing charge and set $g_{i,\tau} = \lambda_i(x_i(\tau))$</li>
  <li>In general, $g_{i,t} = \min_{s\leq t} \lambda_i(x_i(s))$<br />
$g_{i,t}$ is a nonincreasing function of $t$ and only depends on the states of bandit process $i$</li>
</ul>

<p>Consider a simple family of $n$ alternative bandit processes $B_1, \ldots, B_n$. The gambler not only collects $r_{i_t}(x_{i_t}(t))$, but also pays the prevailing charge $g_{i_t,t}$ of the bandit $B_{i_t}$ chosen at time $t$. We have the following observations</p>

<ul>
  <li>
    <p>The gambler cannot do better than break even (i.e., zero expected profit)<br />
reason: a strictly positive profit is only possible if this were to happen for at least one bandit. However, we define the prevailing charge in a way (i.e., always equals the fair charge) such that no profit can be gained from any selected arm.</p>

    <p>Implications</p>

\[\mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t \left( r_{i_t}(x_{i_t}(t)) - g_{i_t}(x_{i_t}(t)) \right) \middle| x(0) \right] \leq 0,\]

    <p>which implies that</p>

\[\mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t r_{i_t}(x_{i_t}(t) \middle| x(0) \right] \leq \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t g_{i_t}(x_{i_t}(t)) \middle| x(0) \right]\]
  </li>
  <li>We maximize the expected discounted sum of prevailing charges by always continuing the bandit with the greatest prevailing charge<br />
Reason
    <ul>
      <li>Each sequence of prevailing charges is nonincreasing</li>
      <li>The above interleaving strategy maximizes the discounted sum</li>
    </ul>

    <p>Example</p>
    <ul>
      <li>$g_1: 10, 10, 9, 5, 5, 3, \ldots$</li>
      <li>$g_2: 20, 15, 7, 4, 2, 2$</li>
      <li>optimal sequence: $20, 15, 10, 10, 9, 7, 5, 5, 4, 3, 2, 2$</li>
    </ul>

    <p>Implication: the Gittins index policy $\pi^*$ maximizes the right-hand side</p>
  </li>
  <li>The Gittins index policy $\pi^*$ of always continuing the bandit with the greatest index allows us to break even.<br />
Reason: the Gittins index policy enables us to earn as much as those indicated by fair charges
Note: if an arm is not pulled, then the state is fixed, and hence the corresponding Gittins index is unchanged.<br />
Implication: the Gittins index policy attains equality</li>
</ul>

<h3 id="alternative-proof-via-interchanging-bandit-portions">Alternative proof via interchanging bandit portions</h3>

<ul>
  <li>Intuition: let a bandit process with a higher reward run first<br />
If in an optimal policy there is a point where we deviate from the arm with the greatest Gittins index, then by interchanging the order we can increase the reward. Hence, this contradiction implies that the optimal policy requires pulling the arm with a high index as early as possible.</li>
</ul>

<h2 id="computational-example">Computational Example</h2>

<ul>
  <li>
    <p>Setup<br />
We have $M$ independent bandit processes. Each process $i$ is endowed with a state $s_i$ belonging to a set $\mathcal{S} = {1, \ldots, N}$ and a known state transition matrix $P_i \in \mathbb{R}^{N \times N}$. At each state $s_i$, there is an associated reward $r_i \in \mathbb{R}$.</p>

    <p>At every time $k$, we will select a bandit process and receive a reward. The state of the selected process transits to a new value, whereas the states of the remaining processes keep fixed.</p>

    <p>Our goal is to maximize the cumulative discounted rewards.</p>
  </li>
  <li>Comparison
    <ul>
      <li>Strategy based on selecting the maximum Gittins index</li>
      <li>Myopic strategy that selects the arm giving the maximum instantaneous reward</li>
    </ul>
  </li>
  <li>
    <p>Codes<br />
See the Colab link <a href="https://colab.research.google.com/drive/1ekJT80VxjemaFC45LdUuIo78Wj_l8b4_?authuser=0#scrollTo=uPGh6ukeiJay&amp;forceEdit=true&amp;sandboxMode=true">here</a></p>
  </li>
  <li>Results<br />
The strategy based on the Gittins index yields a higher reward.
    <div style="text-align: center;">
  <img src="/img/posts/2024-12-5-Gittins/image.png" alt="Convergence curves" style="width: 70%; display: block; margin: 0 auto;" />
</div>
    <!-- ![alt text](/img/posts/2024-12-5-Gittins/image.png) -->
  </li>
</ul>

<h2 id="summary--extension">Summary &amp; Extension</h2>

<ul>
  <li>
    <p>Summary<br />
The Gittins index theorem provides an elegant characterization of the optimal policy for the multi-armed bandit problem with Markovian reward sequences. Instead of dealing with a vast Markov Decision Process, we only need to calculate the Gittins indexes of states of each arm and then obtain the optimal policy.</p>
  </li>
  <li>
    <p>Control<br />
Setup: an unknown dynamical system and a set of candidate controllers</p>

\[\begin{aligned}
&amp;x(t+1) = Ax(t) + Bu(t), \quad u(t) = -K_{i_t}x(t),  \\
&amp;\{K_i|i=1,\ldots,N\} : \mathrm{set~of~candidate~controllers}
\end{aligned}\]

    <p>Challenges</p>
    <ul>
      <li>dependent bandit processes</li>
      <li>the reward is not fixed if the corresponding bandit is left unchosen</li>
    </ul>
  </li>
</ul>]]></content><author><name>Zhiyu He</name></author><summary type="html"><![CDATA[Gittins Index Theorem and Multi-Armed Bandits]]></summary></entry></feed>