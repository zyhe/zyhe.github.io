<!DOCTYPE html>

<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>
    Gittins Index Theorem and Multi-Armed Bandits - Zhiyu He
    
  </title>

  <meta name="description" content="Gittins Index Theorem and Multi-Armed Bandits">

  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://zyhe.github.io/2024/12/05/Gittins-">
  <link rel="alternate" type="application/rss+xml" title="Zhiyu He" href="/feed.xml">

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="/assets/icons/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/assets/icons/android-chrome-192x192.png">
  <link rel="icon" type="image/png" sizes="512x512" href="/assets/icons/android-chrome-512x512.png">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-config" content="/assets/images/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">

  <!-- Include Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Configure MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      loader: {load: ['[tex]/ams']}
    };
  </script>
  <!-- Include MathJax -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="/">Zhiyu He</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/research">Research</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/teaching">Teaching</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/posts">Blog</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


  <!-- Page Header -->

<header class="masthead" style="background-image: url('/img/posts/2024-12-5-Gittins/background.jpg')">
  
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-11 col-md-11 mx-auto">
          <div class="post-heading">
            <h1>Gittins Index Theorem and Multi-Armed Bandits</h1>
            
            <h2 class="subheading">A brief tutorial</h2>
            
            <span class="meta">Posted by
              <a href="#">Zhiyu He</a>
              on December 05, 2024 &middot; <span class="reading-time" title="Estimated read time">
  
   14 mins  read
</span>
            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <div class="container">
    <div class="row">
      <div class="col-lg-11 col-md-11 mx-auto">

        <h1 id="gittins-index-theorem-and-multi-armed-bandits">Gittins Index Theorem and Multi-Armed Bandits</h1>

<p>Zhiyu He, December 5, 2024<br />
Adapted from</p>

<ul>
  <li>Gittins, John, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John Wiley &amp; Sons, 2011.</li>
  <li>Sébastie Bubeck and Nicolò Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning, 2012.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Goal: solve sequential decision-making problems featuring exploration-exploitation trade-offs</li>
  <li>Salient feature: to find an optimal policy, it suffices to focus on an index corresponding to each choice and encompassing immediate rewards and future benefits</li>
</ul>

<h3 id="motivating-example">Motivating Example</h3>

<ul>
  <li>Two-armed bandit<br />
Consider two slot machines that give the following reward sequences
    <ul>
      <li>Slot machine 1: $4, 5, 2, 6, 10, 1, \ldots$</li>
      <li>Slot machine 2: $8, 3, 10, 12, 1, 2, \ldots$</li>
    </ul>

    <p>If not pulled, then the reward to be given is fixed.<br />
How to sequentially select a machine (i.e., an arm) to maximize the discounted cumulative reward?<br />
A myopic solution: $8 + 4a + 5a^2 + 3a^3 + 10a^4 + \ldots$<br />
An arguably better solution $8 + 3a + 10a^2 + 12a^3 + 4a^4 + \ldots$<br />
(For instance, if the discount factor is $a=0.9$, then the latter discounted sum is larger than the former)</p>
  </li>
  <li>Other examples
    <ul>
      <li>Single machine scheduling</li>
      <li>Managing research projects</li>
    </ul>
  </li>
</ul>

<h3 id="three-formulations-of-the-bandit-problem">Three formulations of the bandit problem</h3>

<ul>
  <li>Criterion: the nature of the reward sequence
    <ul>
      <li>stochastic (drawn from some unknown distribution)
-&gt; upper confidence bound strategies<br />
idea: construct upper confidence bound estimates on the means of arms and choose the arm that looks best under these estimates<br />
principle: optimism in face of uncertainty</li>
      <li>adversarial (arbitrarily set after we select the arm)
-&gt; hedge algorithm<br />
idea: maintain and iteratively update a probability distribution of selecting different arms</li>
      <li>Markovian (features known state transition)<br />
-&gt; Gittins index</li>
    </ul>
  </li>
</ul>

<h2 id="problem-formulation">Problem Formulation</h2>

<ul>
  <li>
    <p>Simple family of alternative bandit processes<br />
$n$ independent bandit processes $B_i, i=1,\ldots,N$</p>
  </li>
  <li>two possible actions
    <ul>
      <li>continue: produces reward and causes state transition</li>
      <li>freeze: no reward and the state is fixed</li>
    </ul>
  </li>
  <li>
    <p>state $\xi_i$ lies in a countable state space $E_i$</p>
  </li>
  <li>
    <p>At decision time $t$: apply control to a specific $B_i$</p>
  </li>
  <li>Results
    <ul>
      <li>obtain a reward $r_i(\xi_i)$ from the bandit process $B_i$</li>
      <li>the state of $B_i$ transits from $\xi_i$ to $y$ with probability $P_i(y|\xi_i)$</li>
      <li>other bandits are frozen: their states do not change, and no rewards are obtained</li>
    </ul>
  </li>
  <li>
    <p>Goal: maximize the infinite-horizon expected discounted sum of rewards (i.e., the payoff)</p>

\[\sup_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} a^t r_{i_t} (\xi_{i_t} (t))\right]\]

    <p>$a \in (0,1)$: discount factor   $i_t$: the index of the bandit process chosen at time $t$</p>
  </li>
</ul>

<h3 id="why-not-alternative-methods">Why not alternative methods?</h3>

<ul>
  <li>Dynamic programming: the number of stationary policies (or the size of the state space) grows exponentially with the number of bandits
    <ul>
      <li>
        <p>Value function</p>

\[\quad V(\xi) = \sup_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} a^t r_{i_t} (\xi_{i_t} (t)) \mathrel{\Big|} \xi(0) = \xi \right]\]
      </li>
      <li>
        <p>Bellman’s equation</p>

\[V(\xi) = \max_{i \in \{1, \ldots, n\}} \left\{ r_i(\xi_i) + a \sum_{y \in E_i} P_i(y|\xi_i)V(\xi_1, \ldots, \xi_{i-1}, y, \xi_{i+1}, \ldots, \xi_n) \right\}\]
      </li>
    </ul>

    <p>number of states $\prod_i k_i \triangleq k$   for each state, we need to choose from $n$ actions</p>

    <p>number of stationary policies: $k^n$</p>
  </li>
</ul>

<h2 id="gittins-index-theorem">Gittins Index Theorem</h2>
<!-- ### Intuitions from single machine scheduling -->

<ul>
  <li>
    <p>Key idea: we aim to find an index indicating the behavior of a bandit process. This behavior blends immediate rewards and future benefits.</p>
  </li>
  <li>
    <p>Point of attack: construct a simple reference bandit process</p>
  </li>
</ul>

<h3 id="calibrating-bandit-processes">Calibrating bandit processes</h3>

<ul>
  <li>
    <p>Consider a discrete-time standard bandit process $\Lambda$, which gives a known reward $\lambda$ each time it is continued</p>

    <p>If it is selected at every time, then the cumulative discounted reward is</p>

\[\lambda(1+a+a^2+\ldots) = \frac{\lambda}{1-a}\]

    <p>We want to compare a bandit process $B_i$ and $\Lambda$. Consider the following setting: we continuously apply control to $B_i$ and then follow an optimal policy.</p>

    <p>This policy can switch to $\Lambda$ at some future time $\tau(\tau&gt;0)$</p>

    <ul>
      <li>
        <p>Observation: if an optimal policy causes us to switch at time $\tau$, we will never switch back</p>
      </li>
      <li>
        <p>Reason: the information on $B_i$ at time $\tau + 1$ is the same as at time $\tau$. If applying control to $\Lambda$ is optimal at time $\tau$, then it is also optimal at time $\tau + 1$ and afterwards.</p>
      </li>
    </ul>

    <p>The maximal payoff is</p>

\[\sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) + a^{\tau} \frac{\lambda}{1-a} \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>We want to find $\lambda$ such that it is equally optimal to apply control to either of the two bandit processes initially, i.e.,</p>

\[0 = \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) - (1 - a^{\tau}) \frac{\lambda}{1-a} \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>We note that $\sum_{t=0}^{\tau-1} a^\tau = \frac{1-a^\tau}{1-a}$. Therefore, $\lambda$ satisfies the following equation</p>

\[0 = \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} (a^t r_i(x_i(t)) - a^t \lambda) \mathrel{\Big|} x_i(0) = x_i \right]\]

    <p>The right-hand side is the supremum of decreasing linear functions of $\lambda$. Hence, it is convex and decreasing in $\lambda$. The root exists and is unique. We obtain the following expression of $\lambda$</p>

\[\lambda = \sup_{\tau &gt; 0} \frac{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) \right | x_i(0)=x_i ]}{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t | x_i(0)=x_i \right] }\]

    <p>Another equivalent formulation</p>

\[\sup\left\{\lambda : \sup_{\tau &gt; 0} \mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t [r_i(x_i(t)) - \lambda] \mid x_i(0) = x_i \right] \geq 0\right\}\]

    <p>Reason: the supremum $\lambda$ ensures that the expectation equals zero.</p>

    <p>Interpretation of $\lambda$</p>
    <ul>
      <li>fixed reward that makes us indifferent to which of $B_i$ and $\Lambda$ to continue initially</li>
      <li>fair charge (highest rent) that we are willing to pay to continue $B_i$ for at least one round and then optimally switch</li>
    </ul>

    <p>Observation</p>
    <ul>
      <li>with the supreme $\lambda$ and the optimal stopping time $\tau$, the cumulative discounted rewards collected from these bandit processes are the same</li>
    </ul>

    <p>This $\lambda$ is the so-called Gittins index</p>
  </li>
</ul>

<h3 id="key-statement">Key statement</h3>

<p>We maximize the expected discounted reward obtained from a simple family of alternative bandit processes by always continuing the bandit having greatest Gittins index. Namely, for</p>

\[v(B_i, x_i) = \sup_{\tau &gt; 0} \frac{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t r_i(x_i(t)) \right | x_i(0)=x_i ]}{\mathbb{E} \left[ \sum_{t=0}^{\tau-1} a^t | x_i(0)=x_i \right] }\]

<p>we always select the bandit process $i = \operatorname{argmax}_i v(B_i,x_i)$.</p>

<ul>
  <li>Numerator: discounted reward up to $\tau$ (stopping time)</li>
  <li>Denominator: discounted time up to $\tau$</li>
</ul>

<p>Note that the Gittins index only depends on the corresponding bandit process (or more specifically, $a, r_i, p_i$) but not on other processes.</p>

<h3 id="proof-via-prevailing-charges-arguments">Proof via prevailing charges arguments</h3>

<ul>
  <li>
    <p>High-level idea<br />
(i) Construct an upper bound; (ii) show that this upper bound can be attained<br />
Abstract example<br />
Suppose that we have two functions $f,g$ such that $f(\pi) \leq g(\pi), \forall \pi$. If we find a policy $\pi^<em>$ such that $f(\pi^</em>) = g(\pi^<em>)$ and $g(\pi) \leq g(\pi^</em>), \forall \pi$, then $\pi^*$ maximizes $f(\pi)$</p>
  </li>
  <li>
    <p>Prevailing charge $g_i(x_i)$: fixed charge that we pay to continue playing the bandit $i$ at state $x_i$<br />
If $g_i(x_i)$ is too large, then we will quit playing this bandit</p>
  </li>
  <li>
    <p>Fair charge $\lambda_i(x_i)$: level of prevailing charge when we are indifferent at state $x_i$ between stopping and continuing for at least one more step and then optimally stopping<br />
We give $\lambda_i$ in the previous ratio</p>
  </li>
</ul>

<p>Processes</p>

<ul>
  <li>Initially, $g_{i,0} = \lambda_i(x_i(0))$</li>
  <li>A point will be reached when it would be optimal to stop continuing the bandit $i$. This point corresponds to the stopping time $\tau$.
At this time, $\lambda_i(x_i(\tau)) &lt; g_{i,0} = \lambda_i(x_i(0))$
This is also the first time that the prevailing charge is higher than the fair charge</li>
  <li>We reduce the prevailing charge and set $g_{i,\tau} = \lambda_i(x_i(\tau))$</li>
  <li>In general, $g_{i,t} = \min_{s\leq t} \lambda_i(x_i(s))$<br />
$g_{i,t}$ is a nonincreasing function of $t$ and only depends on the states of bandit process $i$</li>
</ul>

<p>Consider a simple family of $n$ alternative bandit processes $B_1, \ldots, B_n$. The gambler not only collects $r_{i_t}(x_{i_t}(t))$, but also pays the prevailing charge $g_{i_t,t}$ of the bandit $B_{i_t}$ chosen at time $t$. We have the following observations</p>

<ul>
  <li>
    <p>The gambler cannot do better than break even (i.e., zero expected profit)<br />
reason: a strictly positive profit is only possible if this were to happen for at least one bandit. However, we define the prevailing charge in a way (i.e., always equals the fair charge) such that no profit can be gained from any selected arm.</p>

    <p>Implications</p>

\[\mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t \left( r_{i_t}(x_{i_t}(t)) - g_{i_t}(x_{i_t}(t)) \right) \middle| x(0) \right] \leq 0,\]

    <p>which implies that</p>

\[\mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t r_{i_t}(x_{i_t}(t) \middle| x(0) \right] \leq \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} a^t g_{i_t}(x_{i_t}(t)) \middle| x(0) \right]\]
  </li>
  <li>We maximize the expected discounted sum of prevailing charges by always continuing the bandit with the greatest prevailing charge<br />
Reason
    <ul>
      <li>Each sequence of prevailing charges is nonincreasing</li>
      <li>The above interleaving strategy maximizes the discounted sum</li>
    </ul>

    <p>Example</p>
    <ul>
      <li>$g_1: 10, 10, 9, 5, 5, 3, \ldots$</li>
      <li>$g_2: 20, 15, 7, 4, 2, 2$</li>
      <li>optimal sequence: $20, 15, 10, 10, 9, 7, 5, 5, 4, 3, 2, 2$</li>
    </ul>

    <p>Implication: the Gittins index policy $\pi^*$ maximizes the right-hand side</p>
  </li>
  <li>The Gittins index policy $\pi^*$ of always continuing the bandit with the greatest index allows us to break even.<br />
Reason: the Gittins index policy enables us to earn as much as those indicated by fair charges
Note: if an arm is not pulled, then the state is fixed, and hence the corresponding Gittins index is unchanged.<br />
Implication: the Gittins index policy attains equality</li>
</ul>

<h3 id="alternative-proof-via-interchanging-bandit-portions">Alternative proof via interchanging bandit portions</h3>

<ul>
  <li>Intuition: let a bandit process with a higher reward run first<br />
If in an optimal policy there is a point where we deviate from the arm with the greatest Gittins index, then by interchanging the order we can increase the reward. Hence, this contradiction implies that the optimal policy requires pulling the arm with a high index as early as possible.</li>
</ul>

<h2 id="computational-example">Computational Example</h2>

<ul>
  <li>
    <p>Setup<br />
We have $M$ independent bandit processes. Each process $i$ is endowed with a state $s_i$ belonging to a set $\mathcal{S} = {1, \ldots, N}$ and a known state transition matrix $P_i \in \mathbb{R}^{N \times N}$. At each state $s_i$, there is an associated reward $r_i \in \mathbb{R}$.</p>

    <p>At every time $k$, we will select a bandit process and receive a reward. The state of the selected process transits to a new value, whereas the states of the remaining processes keep fixed.</p>

    <p>Our goal is to maximize the cumulative discounted rewards.</p>
  </li>
  <li>Comparison
    <ul>
      <li>Strategy based on selecting the maximum Gittins index</li>
      <li>Myopic strategy that selects the arm giving the maximum instantaneous reward</li>
    </ul>
  </li>
  <li>
    <p>Codes<br />
See the Colab link <a href="https://colab.research.google.com/drive/1ekJT80VxjemaFC45LdUuIo78Wj_l8b4_?authuser=0#scrollTo=uPGh6ukeiJay&amp;forceEdit=true&amp;sandboxMode=true">here</a></p>
  </li>
  <li>Results<br />
The strategy based on the Gittins index yields a higher reward.
    <div style="text-align: center;">
  <img src="/img/posts/2024-12-5-Gittins/image.png" alt="Convergence curves" style="width: 70%; display: block; margin: 0 auto;" />
</div>
    <!-- ![alt text](/img/posts/2024-12-5-Gittins/image.png) -->
  </li>
</ul>

<h2 id="summary--extension">Summary &amp; Extension</h2>

<ul>
  <li>
    <p>Summary<br />
The Gittins index theorem provides an elegant characterization of the optimal policy for the multi-armed bandit problem with Markovian reward sequences. Instead of dealing with a vast Markov Decision Process, we only need to calculate the Gittins indexes of states of each arm and then obtain the optimal policy.</p>
  </li>
  <li>
    <p>Control<br />
Setup: an unknown dynamical system and a set of candidate controllers</p>

\[\begin{aligned}
&amp;x(t+1) = Ax(t) + Bu(t), \quad u(t) = -K_{i_t}x(t),  \\
&amp;\{K_i|i=1,\ldots,N\} : \mathrm{set~of~candidate~controllers}
\end{aligned}\]

    <p>Challenges</p>
    <ul>
      <li>dependent bandit processes</li>
      <li>the reward is not fixed if the corresponding bandit is left unchosen</li>
    </ul>
  </li>
</ul>


        <hr>

        <div class="clearfix">

          
          
          <a class="btn btn-primary float-right" href="/2025/02/02/journey-to-the-west.html" data-toggle="tooltip" data-placement="top" title="西游之悟：灵山只在汝心头">Next<span class="d-none d-md-inline">
              Post</span> &rarr;</a>
          

        </div>

      </div>
    </div>
  </div>


  <!-- Footer -->

<hr>

<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          
          <li class="list-inline-item">
            <a href="mailto:zhiyhe(at)ethz.ch">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="far fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          
          
          <li class="list-inline-item">
            <a href="https://www.linkedin.com/in/zhiyu-he-ba518a225">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://github.com/zyhe">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://scholar.google.com/citations?user=6F5yIIEAAAAJ">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-graduation-cap fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
        </ul>
        <p class="copyright text-muted">Copyright &copy; Zhiyu He 2025</p>
      </div>
    </div>
  </div>
</footer>


  <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="/assets/vendor/startbootstrap-clean-blog/js/scripts.js"></script>

<script src="/assets/scripts.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-C0YTJSLE5C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-C0YTJSLE5C');
</script>



</body>

</html>
